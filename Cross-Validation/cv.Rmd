---
output: pdf_document
---

The 5 algorithms I choose are: `Generalized Linear Models`, `Support Vector Machines`, `Generalized Boosted Models`, `Cart` and `Random Forests`. We can see from the graphs generated below with R code, these 5 algorithms provide similar ROC curves. Also, when doing nested cross validation, the 2 algorithms in 10 folds also provide similar ROC curves.

I take `Channel` as `y` and run the 5 algorithms, 2 nested cross validation for 10 folds and ROC curves of individual feature for 10 test folds.

By plotting ROC curves and computing AUC of individual feature, the graphs show that the some of the features have a sort of classification abilities. I run 5 algorithms and plot the ROC curves, considering all the features, the AUC of 5 algorithms all larger than single feature. This result represents by using 5 algorithms, we can find better classifiers. The algorithm is better than indivial feature because it considers combination features.

By running nested cross validation, the AUC of 2 chosen algorithms are lager than the AUC generated by 5 algorithms. This means nested cross validation find better classifier than 5 algorithms. The nested cross validation is better than single algorithm because it finds the optimal parameter to run the algorithm.


```{r message=FALSE, include=FALSE, cache=TRUE}
library(gbm)
library(plyr)
library(dplyr)
library(rpart)
library(lattice)
library(ggplot2)
library(pROC)
library(caret)
library(readr)
```

Dataset: Wholesale customers Data Set

https://archive.ics.uci.edu/ml/datasets/Wholesale+customers

Attribute Information:

1)	FRESH: annual spending (m.u.) on fresh products (Continuous); 

2)	MILK: annual spending (m.u.) on milk products (Continuous); 

3)	GROCERY: annual spending (m.u.) on grocery products (Continuous); 

4)	FROZEN: annual spending (m.u.) on frozen products (Continuous) 

5)	DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous) 

6)	DELICATESSEN: annual spending (m.u.) on and delicatessen products (Continuous); 

7)	CHANNEL: customers Channel - Horeca or Retail channel (Binary)

8)	REGION: customers Region - Lisnon, Oporto or Other (Nominal)

Take binary Channel as the `y`. Recode the 1 into `no` and 2 into `yes`, which denotes as whether or not the costomers are from channel 2.

```{r message=FALSE, cache=TRUE, results='hide'}
wholesale <- read_csv("/Wholesale customers data.csv")
wholesale$Class <- wholesale$Channel
wholesale <- subset(wholesale, select = -c(Channel))
# Whether or not the customers are from channel 2
wholesale$Class[wholesale$Class == 1] <- 0
wholesale$Class[wholesale$Class == 2] <- 1
wholesale$Class <- factor(wholesale$Class, levels=c(0, 1), labels=c('no', 'yes'))    
attributes(wholesale$Class)

set.seed(1984)

# 4/5 of data for training, 1/5 of data for evaluation/testing
training <- createDataPartition(wholesale$Class, p = 0.8, list = FALSE)
trainData <- wholesale[training,]
testData <- wholesale[-training,]
```

### 5 different algorithms
```{r message=FALSE, cache=TRUE, results='hide'}
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 10,
                           # Estimate class probabilities
                           classProbs = TRUE,
                           # Evaluate performance using 
                           # the following function
                           summaryFunction = twoClassSummary)
## 5 algorithm
set.seed(2014)

# 1. Fit a glm model (logistic regression) and score the test data set
glmModel <- glm(Class~ . , data = trainData, family = binomial)
pred.glmModel <- as.vector(predict(glmModel, newdata = testData,
                                   type = "response"))
  # Calculate the AUC for the test data set.
roc.glmModel <- pROC::roc(testData$Class, pred.glmModel)
auc.glmModel <- pROC::auc(roc.glmModel)

# 2. Support Vector Machines
svmModel <- train(Class ~ ., data = trainData, method = "svmRadial", metric = "ROC",
                  trControl = fitControl, verbose = FALSE, tuneLength = 5)
pred.svmModel <- as.vector(predict(svmModel, newdata = testData
                                   , type = "prob")[, "yes"])
  # Calculate the AUC for the test data set.
roc.svmModel <- pROC::roc(testData$Class, pred.svmModel)
auc.svmModel <- pROC::auc(roc.svmModel)

# 3. Boosted Trees
gbmModel <- train(Class ~ ., data = trainData, method = "gbm", metric = "ROC",
                  trControl = fitControl, verbose = FALSE, tuneLength = 5)
pred.gbmModel <- as.vector(predict(gbmModel, newdata = testData,
                                   type = "prob")[, "yes"])
  # Calculate the AUC for the test data set.
roc.gbmModel <- pROC::roc(testData$Class, pred.gbmModel)
auc.gbmModel <- pROC::auc(roc.gbmModel)

# 4. CART
cartModel <- train(Class ~ ., data = trainData, method = "rpart", metric = "ROC",
                   trControl = fitControl, tuneLength = 5)
  # Loading required package: rpart
pred.cartModel <- as.vector(predict(cartModel, newdata = testData,
                                    type = "prob")[, "yes"])
  # Calculate the AUC for the test data set.
roc.cartModel <- pROC::roc(testData$Class, pred.cartModel)
auc.cartModel <- pROC::auc(roc.cartModel)

# 5. Random Forest
rfModel <- train(Class ~ ., data = trainData, method = "rf", metric = "ROC",
                 trControl = fitControl, verbose = FALSE, tuneLength = 5)
  # Loading required package: randomForest
  # randomForest 4.6-10
  # Type rfNews() to see new features/changes/bug fixes.
pred.rfModel <- as.vector(predict(rfModel, newdata = testData,
                                  type = "prob")[, "yes"])
  # Calculate the AUC for the test data set.
roc.rfModel <- pROC::roc(testData$Class, pred.rfModel)
auc.rfModel <- pROC::auc(roc.rfModel)
```

```{r message=FALSE, cache=TRUE, results='hide'}
## Choose the best model
# Plot AUC, on the test data set, for each model.
test.auc <- data.frame(model = c("glm", "svm", "gbm", "cart", "rForest"),
                       auc = c(auc.glmModel, auc.svmModel, auc.gbmModel,
                               auc.cartModel, auc.rfModel))
test.auc <- test.auc[order(test.auc$auc, decreasing = TRUE),]
test.auc$model <- factor(test.auc$model, levels = test.auc$model)
test.auc

library(ggplot2)
theme_set(theme_gray(base_size = 18))
ggplot(test.auc, aes(x = model, y = auc)) +
  geom_bar(stat = "identity") +
  ylim(0, 1) +
  geom_text(aes(label = round(test.auc$auc, digits = 3)), vjust = -0.25)

# plot(glmModel)
plot(roc.glmModel, print.auc = TRUE, print.auc.x = 0.7, print.auc.y = 0.3,
     print.auc.col = "blue", type = "l", col = 'blue', lwd = 1, lty = 1)
# plot(svm.rocCurve)
plot(roc.svmModel, print.auc = TRUE, print.auc.x = 0.7, print.auc.y = 0.25,
     print.auc.col = "purple", type = "l", add = TRUE, col = 'purple', lwd = 1, lty = 1)
# plot(gbmModel)
plot(roc.gbmModel, print.auc = TRUE, print.auc.x = 0.7, print.auc.y = 0.2,
     print.auc.col = "green", type = "l", add = TRUE, col = 'green', lwd = 1, lty = 1)
# plot(cartModel)
plot(roc.cartModel, print.auc = TRUE, print.auc.x = 0.7, print.auc.y = 0.15,
     print.auc.col = "orange", type = "l", add = TRUE, col = 'orange', lwd = 1, lty = 1)
# plot(rfModel)
plot(roc.rfModel, print.auc = TRUE, print.auc.x = 0.7, print.auc.y = 0.1,
     print.auc.col = "navy", type = "l", add = TRUE, col = 'navy', lwd = 1, lty = 1)
legend("bottomright",
       legend = c("logistic regression", "SVM", "Boosted Trees", "CART", "rForest"),
       col = c("blue", "purple", "green", "orange", "navy"), lwd = 1)
```

### Perform 10 fold nested cross validation for one parameter
```{r message=FALSE, cache=TRUE, results='hide'}
wholesale <- read_csv("/Wholesale customers data.csv")
wholesale$Class <- wholesale$Channel
wholesale <- subset(wholesale, select = -c(Channel))
# Whether import the commodities from channel 2
wholesale$Class[wholesale$Class == 1] <- 0
wholesale$Class[wholesale$Class == 2] <- 1
wholesale$Class <- factor(wholesale$Class, levels=c(0, 1), labels=c('no', 'yes'))    
attributes(wholesale$Class)

# Nested Cross Validation
# 4/5 of data for training,
# 1/10 of data for validation,
# 1/10 of data for evaluation/testing
# Shuffle the data first (to make sure the folds are randomly selected)
n <- nrow(wholesale)/10
i = 1
wholesale_tmp <- wholesale
wholesale_tmp <- wholesale_tmp[sample(nrow(wholesale_tmp)), ]

# Divide data into 10 folds
while(i <= 10) {
  fold <- paste("fold", i, sep="")
  assign(fold, wholesale_tmp[1:n,])
  wholesale_tmp <- wholesale_tmp[-(1:n),]
  i=i+1
}
```

```{r message=FALSE, cache=TRUE, results='hide'}
# Assign parameters
K1 <- c(0.01, 0.1, 0.3, 0.5, 1) # Parameter for SVM
K2 <- c(0.01, 0.1, 0.3, 0.5, 1) # Parameter for Cart

nfitControl <- trainControl(method = "repeatedcv",
                            number = 10,
                            repeats = 10,
                            # Estimate class probabilities
                            classProbs = TRUE,
                            # Evaluate performance using
                            # the following function
                            summaryFunction = twoClassSummary)

mean.valid.auc <- data.frame(model = c("svm", "cart"), k1 = rep(NA, 2), k2 = rep(NA, 2),
                             k3 = rep(NA, 2), k4 = rep(NA, 2), k5 = rep(NA, 2))
valid.auc <- data.frame(auc1 = rep(NA, 2), auc2 = rep(NA, 2), auc3 = rep(NA, 2),
                        auc4 = rep(NA, 2), auc5 = rep(NA, 2), auc6 = rep(NA, 2),
                        auc7 = rep(NA, 2), auc8 = rep(NA, 2), auc9 = rep(NA, 2))
test <- data.frame(model = c("svm", "cart"), test1 = rep(NA, 2), test2 = rep(NA, 2),
                   test3 = rep(NA, 2), test4 = rep(NA, 2), test5 = rep(NA, 2),
                   test6 = rep(NA, 2), test7 = rep(NA, 2), test8 = rep(NA, 2),
                   test9 = rep(NA, 2), test10 = rep(NA, 2))

set.seed(2014)
for (j in 1:10) {
  # Choose 1 fold from 10 folds to be test fold
  ntestData <- eval(as.symbol(paste("fold", j, sep = "")))
  for (i in 1:5) {
    n = 1
    for (m in 1:10) {
      if (m != j) {
        j0 = 1+(j-1)*44
        j1 = j*44
        m0 = 1+(m-1)*44
        m1 = m*44
        # Choose 1 fold from the rest 9 to be the valid fold
        nvalidData <- eval(as.symbol(paste("fold", m, sep="")))
        # Exclude the 2 folds chosen to be test and valid folds,
        # use the rest folds to be the training data
        ntrainData <- wholesale[-c(j0:j1, m0:m1),]
        # Support Vector Machines
        nsvmModel <- train(Class ~ ., data = ntrainData, method = "svmRadial", metric = "ROC",
                           trControl = nfitControl, verbose = FALSE, tuneLength = 5,
                           tuneGrid = data.frame(.C = K1[i], .sigma = .05))
        npred.svmModel <- as.vector(predict(nsvmModel, newdata = nvalidData,
                                            type = "prob")[, "yes"])
        nroc.svmModel <- pROC::roc(nvalidData$Class, npred.svmModel)
        nauc.svmModel <- pROC::auc(nroc.svmModel)
       
        # CART
        ncartModel <- train(Class ~ ., data = ntrainData, method = "rpart", metric = "ROC",
                            trControl = nfitControl, tuneLength = 5,
                            tuneGrid = data.frame(.cp = K2[i]),
                            control = rpart.control(minsplit = 10, minbucket = 5))
        npred.cartModel <- as.vector(predict(ncartModel, newdata = nvalidData,
                                             type = "prob")[, "yes"])
        nroc.cartModel <- pROC::roc(nvalidData$Class, npred.cartModel)
        nauc.cartModel <- pROC::auc(nroc.cartModel)
        
        # valid.auc stores the auc from 9 times CV per parameter
        auc.name <- paste("auc", n, sep = "")
        valid.auc[[auc.name]] <- c(nauc.svmModel, nauc.cartModel)
        n = n + 1
      }
    }
    # mean.valid.auc computes the mean of the 9 valid.auc for each parameter
    Kpar <- paste("k", i, sep="")
    mean.valid.auc[[Kpar]] <- c(rowMeans(valid.auc[1, ]), rowMeans(valid.auc[2, ]))
  }
  # test selects the maximum in mean.valid.auc and repeats 10 times
  test.name <- paste("test", j, sep = "")
  test[[test.name]][[1]] <- max(which(mean.valid.auc[1, 2:6] ==
                                      max(mean.valid.auc[1, 2:6])))
  test[[test.name]][[2]] <- max(which(mean.valid.auc[2, 2:6] ==
                                      max(mean.valid.auc[2, 2:6])))

}
test.auc <- data.frame(auc1 = rep(NA, 2), auc2 = rep(NA, 2), auc3 = rep(NA, 2),
                       auc4 = rep(NA, 2), auc5 = rep(NA, 2), auc6 = rep(NA, 2),
                       auc7 = rep(NA, 2), auc8 = rep(NA, 2), auc9 = rep(NA, 2))
```


```{r fig.align = 'center', message=FALSE, cache=TRUE, results='hide'}
set.seed(2014)
par(mfrow = c(2, 2))
for (j in 1:10) {
  j0 = 1+(j-1)*44
  j1 = j*44
  ttestData <- eval(as.symbol(paste("fold", j, sep = "")))
  ttrainData <- wholesale[-c(j0:j1),]
  test.name <- paste("test", j, sep = "")
  # Support Vector Machines
  tsvmModel <- train(Class ~ ., data = ttrainData, method = "svmRadial", metric = "ROC",
                    trControl = nfitControl, verbose = FALSE, tuneLength = 5,
                    tuneGrid = data.frame(.C = K1[test[[test.name]][[1]]], .sigma = .05))
  tpred.svmModel <- as.vector(predict(tsvmModel, newdata = ttestData,
                                      type = "prob")[, "yes"])
  troc.svmModel <- pROC::roc(ttestData$Class, tpred.svmModel)
  tauc.svmModel <- pROC::auc(troc.svmModel)
  
  # CART
  tcartModel <- train(Class ~ ., data = ttrainData, method = "rpart", metric = "ROC",
                     trControl = nfitControl, tuneLength = 5,
                     tuneGrid = data.frame(.cp = K2[test[[test.name]][[2]]]),
                     control = rpart.control(minsplit = 10, minbucket = 5))
  tpred.cartModel <- as.vector(predict(tcartModel, newdata = ttestData,
                                       type = "prob")[, "yes"])
  troc.cartModel <- pROC::roc(ttestData$Class, tpred.cartModel)
  tauc.cartModel <- pROC::auc(troc.cartModel)
  
  auc.name <- paste("auc", j, sep = "")
  test.auc[[auc.name]] <- c(tauc.svmModel, tauc.cartModel)
  
  # Mean and SD
  svm.mean <- rowMeans(test.auc[1, ])
  svm.sd <- apply(test.auc[1, ], 1, sd)
  cart.mean <- rowMeans(test.auc[2, ])
  cart.sd <- apply(test.auc[2, ], 1, sd)

  # plot(svm.rocCurve)
  plot(troc.svmModel, print.auc = TRUE, print.auc.x = 0.7, print.auc.y = 0.25,
       print.auc.col = "purple", type = "l", col = 'purple', lwd = 1, lty = 1,
       main = paste("fold", j))
  # plot(cartModel)
  plot(troc.cartModel, print.auc = TRUE, print.auc.x = 0.7, print.auc.y = 0.15,
       print.auc.col = "orange", type = "l", add = TRUE, col = 'orange', lwd = 1, lty = 1)
  legend("bottomright", legend = c("SVM", "CART"),
         col = c("orange", "navy"), lwd = 1, cex = 0.5)
}
```
The AUC mean of SVM is `r svm.mean`, the standard deviation of SVM is `r svm.sd`. Thus, the confidential interval of AUC by using SVM is [`r svm.mean-svm.sd`, `r svm.mean+svm.sd`].

The AUC mean of Cart is `r cart.mean`, the standard deviation of Cart is `r cart.sd`. Thus, the confidential interval of AUC by using Cart is [`r cart.mean-cart.sd`, `r cart.mean+cart.sd`].

After running nested cross validation and find the optimal parameter for the evaluation, the algorithms give better results, that is larger AUCs, compare to the results in question 4.1.

Note, the AUC in 4.1 is `r auc.svmModel` for SVM algorithm and `r auc.cartModel` for Cart algorithm.


```{r fig.align = 'center', message=FALSE, cache=TRUE, results='hide'}
set.seed(2014)
tfitControl <- trainControl(method = "repeatedcv",
                            number = 10,
                            repeats = 10,
                            # Estimate class probabilities
                            classProbs = TRUE,
                            # Evaluate performance using
                            # the following function
                            summaryFunction = twoClassSummary)
par(mfrow = c(1, 2))
for (j in 1:10) {
  ftestData <- eval(as.symbol(paste("fold", j, sep = "")))
  for (i in c("Region", "Fresh", "Milk", "Grocery",
              "Frozen", "Detergents_Paper", "Delicassen")) {
    # Generate the data frame which contains only "Class" and "Feature""
    ftest <- data.frame(feature = ftestData[[i]], Class = ftestData$Class)
    
    # Support Vector Machines
    feature.roc <- roc(ftest$Class, ftest$feature)
    assign(paste("feature.roc", i, sep = ""), feature.roc)
  }
  
  # Plot (put all the features in 1 graph, generated by each fold)
  # plot(svm.rocCurve)
  plot(feature.rocRegion, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.4,
       print.auc.col = "purple", type = "l", col = 'purple', lwd = 1, lty = 1,
       main = paste("fold", j))
  plot(feature.rocFresh, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.3,
       print.auc.col = "blue", type = "l", add = TRUE, col = 'blue', lwd = 1, lty = 1,
       main = paste("fold", j))
  plot(feature.rocMilk, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.2,
       print.auc.col = "green", type = "l", add = TRUE, col = 'green', lwd = 1, lty = 1,
       main = paste("fold", j))
  plot(feature.rocGrocery, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0.1,
       print.auc.col = "orange", type = "l", add = TRUE, col = 'orange', lwd = 1, lty = 1,
       main = paste("fold", j))
  plot(feature.rocFrozen, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = 0,
       print.auc.col = "yellow", type = "l", add = TRUE, col = 'yellow', lwd = 1, lty = 1,
       main = paste("fold", j))
  plot(feature.rocDetergents_Paper, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = -0.1,
       print.auc.col = "maroon", type = "l", add = TRUE, col = 'maroon', lwd = 1, lty = 1,
       main = paste("fold", j))
  plot(feature.rocDelicassen, print.auc = TRUE, print.auc.x = 0.5, print.auc.y = -0.2,
       print.auc.col = "navy", type = "l", add = TRUE, col = 'navy', lwd = 1, lty = 1,
       main = paste("fold", j))
}
```
```{r echo=FALSE}
plot.new()
legend("center",
      legend = c("Region", "Fresh", "Milk", "Grocery", "Frozen",
                "Detergents Paper","Delicassen"),
      col = c("purple", "blue", "green", "orange", "yellow", "maroon", "navy"),
      lty = 1, bty ='n', cex = 0.8)
```
